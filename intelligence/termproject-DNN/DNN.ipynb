{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term project - MNIST visualization<br>\n",
    "Deep neural network 구현 및 Visualization과 이를 통한 Neural network에 대한 이해 향상\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset 불러 오기 및 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. data에 대한 pca(2d라? 겹치는 경유 확인가능) + tsne결과 확인하기\n",
    "    둘의 차이 확인하기\n",
    "2. layer들의 dimension 확인\n",
    "\n",
    "• Input image에 대한 PCA, t-SNE 결과\n",
    "• hidden vector (a[0], z[1], a[1], z[2], a[2])에 대한 PCA, ,t-SNE결과\n",
    "    ->  총 10장\n",
    "• 결과 분석 (자유롭게)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "# dataset\n",
    "import torchvision \n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# sklearn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "modeling 과정에서 직접 설정하는 값입니다.<br>\n",
    "- batch_size\n",
    "- learning rate : 학습률(gradient를 통해 weight를 이동, 변화시키는 정도)\n",
    "- epoch_nums : 학습을 반복하는 횟수\n",
    "- batch size : 한번의 batch마다 주어지는 data sample의 size (batch: 나누어진 dataset)\n",
    "- momentum : weight의 update과정에서 관성, 가속도를 주어 감소, 증가하는 방향으로 더 많은 변화를 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "learning_rate = 0.01\n",
    "epoch_num = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "### torchvision.datasets.MNIST = (root=\"\", train=\"\", dawnload=\"\",trasform =\"\")\n",
    "root = \"data의 경로\" <br>\n",
    "train = True(학습용 데이터) or False(test용 데이터) <br>\n",
    "tramsform - 데이터 형태 <br>\n",
    "download = True(MNIST dataset이 없으면 다운로드한다.) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root= '..\\datasets', train= True, download= True, transform= transforms.ToTensor())\n",
    "test_data = datasets.MNIST(root= '..\\datasets', train= False, download= True, transform= transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "input layer와 ouput layer사이에 hidden layer를 2개이상 지닌 neural network을 의미합니다.\n",
    "- 분류와 수치예측을 주목적으로 하여 이미지 학습과 문자인식에 유용하게 사용되고 있는 신경망입니다.\n",
    "\n",
    "## Using squential \n",
    "squential하여 내부  module에 접급하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ModuleList\n",
    "Module을 리스트 형태로 담을 때 사용합니다. ModuleList에서는 module하나하나에 접근이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ModuleList\n",
    "class DeepNN_ML(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN_ML, self).__init__()\n",
    "\n",
    "        self.in_dim = 28 * 28 # MNIST\n",
    "        self.out_dim = 10 # 0 ~ 9\n",
    "        self.az = [ [] for i in range(10)]\n",
    "        \n",
    "        self.list = nn.ModuleList([\n",
    "            nn.Linear(in_features = self.in_dim, out_features = 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 512, out_features = 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 256, out_features = 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 128, out_features = 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 64, out_features = self.out_dim)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x, V):\n",
    "        fw = x.view(-1, self.in_dim)\n",
    "        if V > 5000: \n",
    "            for i, module in enumerate(self.list):\n",
    "                fw = module(fw)\n",
    "        else:\n",
    "            self.az[0].append(fw.tolist())\n",
    "            for i, module in enumerate(self.list):\n",
    "                fw = module(fw)\n",
    "                self.az[i+1].append(fw.tolist())\n",
    "        return fw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.209\n",
      "[1,  4000] loss: 0.665\n",
      "[2,  2000] loss: 0.305\n",
      "[2,  4000] loss: 0.224\n",
      "[3,  2000] loss: 0.157\n",
      "[3,  4000] loss: 0.140\n",
      "[4,  2000] loss: 0.109\n",
      "[4,  4000] loss: 0.099\n",
      "[5,  2000] loss: 0.080\n",
      "[5,  4000] loss: 0.077\n",
      "[6,  2000] loss: 0.064\n",
      "[6,  4000] loss: 0.061\n",
      "[7,  2000] loss: 0.045\n",
      "[7,  4000] loss: 0.050\n",
      "[8,  2000] loss: 0.039\n",
      "[8,  4000] loss: 0.041\n",
      "[9,  2000] loss: 0.031\n",
      "[9,  4000] loss: 0.030\n",
      "[10,  2000] loss: 0.023\n",
      "[10,  4000] loss: 0.026\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "model_ML = DeepNN_ML()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_ML.parameters(), lr = learning_rate)\n",
    "label_list = []\n",
    "V = (60000/batch_size) * epoch_num\n",
    "for epoch in range(epoch_num):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        # get the inputs: data is a list of [input, label]\n",
    "        inputs, labels = data\n",
    "        if epoch == epoch_num-1:\n",
    "            label_list.append(labels.tolist())\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward, backward, optimizer\n",
    "        outputs = model_ML(inputs, V)\n",
    "        loss = criterion(outputs, labels) # loss\n",
    "        loss.backward() # backward\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 2000 == 0 : # print every 2000 mini-batch\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch+1, i+1, running_loss/2000) )\n",
    "            running_loss = 0.0\n",
    "        V -= 1\n",
    "print(\"Finished training\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a699302accf0dd49589cc59e863a793f2e26d9be8786b40f18cd587a7dd1ecfb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('ai21': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
